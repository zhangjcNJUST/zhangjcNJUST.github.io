<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>点云投影(1)--投影到XY平面(鸟瞰图)</title>
    <url>/2020/01/31/projection1/</url>
    <content><![CDATA[<p>参考：<a href="https://blog.csdn.net/qq_33801763/article/details/78923310" target="_blank" rel="noopener">处理点云数据(一)：点云与生成鸟瞰图</a></p>
<h1 id="1-图像坐标系与激光雷达坐标系"><a href="#1-图像坐标系与激光雷达坐标系" class="headerlink" title="1. 图像坐标系与激光雷达坐标系"></a>1. 图像坐标系与激光雷达坐标系</h1><p><img src="/2020/01/31/projection1/1.png" alt="1" style="zoom: 33%;"></p>
<p>图像坐标系：</p>
<ul>
<li>原点左上角</li>
<li>坐标值都为正</li>
<li>坐标值都是整数</li>
</ul>
<p><img src="/2020/01/31/projection1/2.png" alt="2" style="zoom:33%;"></p>
<p>点云坐标系：</p>
<ul>
<li>左方为Y轴正方向、前方为X轴正方向、上方为Z轴正方向</li>
<li>坐标值有正有负</li>
<li>坐标值为实数</li>
</ul>
<h1 id="2-创建鸟瞰图投影"><a href="#2-创建鸟瞰图投影" class="headerlink" title="2. 创建鸟瞰图投影"></a>2. 创建鸟瞰图投影</h1><h2 id="2-1-选择投影区域"><a href="#2-1-选择投影区域" class="headerlink" title="2.1 选择投影区域"></a>2.1 选择投影区域</h2><p>一般情况下，只关注车体附近一定范围内的点云，无需对所有点云进行处理，本文选择车身左右各10m，车后20m，车前40m作为感兴趣区域。</p>
<h2 id="2-2-设置投影分辨率"><a href="#2-2-设置投影分辨率" class="headerlink" title="2.2 设置投影分辨率"></a>2.2 设置投影分辨率</h2><p>设置每个像素所代表的区域范围，本文设置分辨率res为0.05m，则生成投影图的高度为$ 60 / 0.05 = 1200$，宽度为$20 / 0.05 = 400$。</p>
<h2 id="2-3-将每个点映射到像素位置"><a href="#2-3-将每个点映射到像素位置" class="headerlink" title="2.3 将每个点映射到像素位置"></a>2.3 将每个点映射到像素位置</h2><p>映射公式：</p>
<script type="math/tex; mode=display">
col = [-y / res] + \lfloor leftoffset / res \rfloor</script><script type="math/tex; mode=display">
row = [-x / res] + \lceil forwardoffset / res \rceil</script><p>其中，$leftoffset$和$forwardoffset$为车左方和前方的最大距离，均为正值，目的是是将转换后的原点变为左上角，坐标值都为正。</p>
<h2 id="2-4-填充像素值"><a href="#2-4-填充像素值" class="headerlink" title="2.4 填充像素值"></a>2.4 填充像素值</h2><p>将所有点的Z值归一化到0-255后，赋值给每个点对应位置的像素值。</p>
<h1 id="3-可视化结果"><a href="#3-可视化结果" class="headerlink" title="3. 可视化结果"></a>3. 可视化结果</h1><p><img src="/2020/01/31/projection1/3.png" alt="3" style="zoom:200%;"></p>
<h1 id="4-相关代码"><a href="#4-相关代码" class="headerlink" title="4. 相关代码"></a>4. 相关代码</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_velodyne_points</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">""" load lidar data from .bin file </span></span><br><span class="line"><span class="string">        [https://github.com/hunse/kitti]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    points = np.fromfile(filename, dtype = np.float32).reshape(<span class="number">-1</span>,<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># remove intensity</span></span><br><span class="line">    points = points[:,<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">return</span> points</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    data_file = <span class="string">"./data.bin"</span></span><br><span class="line">    points = load_velodyne_points(data_file)</span><br><span class="line">    <span class="comment">#print(points.shape) #55597*3</span></span><br><span class="line">    <span class="comment">#print(points[:20])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># choose ROI</span></span><br><span class="line">    <span class="comment"># left 10,right 10,back 20.forward 40</span></span><br><span class="line">    side_range = (<span class="number">-10</span>, <span class="number">10</span>)</span><br><span class="line">    fwd_range = (<span class="number">-20</span>, <span class="number">40</span>)</span><br><span class="line">    </span><br><span class="line">    x_points = points[:, <span class="number">0</span>]</span><br><span class="line">    y_points = points[:, <span class="number">1</span>]</span><br><span class="line">    z_points = points[:, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filter</span></span><br><span class="line">    side_filter = np.logical_and((y_points &gt; -side_range[<span class="number">1</span>]), (y_points &lt; -side_range[<span class="number">0</span>])) <span class="comment"># left y positive, right y negative</span></span><br><span class="line">    fwd_filter = np.logical_and((x_points &gt; fwd_range[<span class="number">0</span>]), (x_points &lt; fwd_range[<span class="number">1</span>]))</span><br><span class="line">    filter = np.logical_and(side_filter, fwd_filter)</span><br><span class="line">    indices = np.argwhere(filter).flatten()</span><br><span class="line"></span><br><span class="line">    x_points = x_points[indices]</span><br><span class="line">    y_points = y_points[indices]</span><br><span class="line">    z_points = z_points[indices]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(x_points.size)</span></span><br><span class="line">    <span class="comment"># print(y_points.size)</span></span><br><span class="line">    <span class="comment"># print(z_points.size)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># resolution</span></span><br><span class="line">    reso = <span class="number">0.05</span> <span class="comment"># 5cm</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># transfer to image axis</span></span><br><span class="line">    x_img = (-y_points / reso).astype(np.int32)</span><br><span class="line">    y_img = (-x_points / reso).astype(np.int32)</span><br><span class="line">    x_img -= int(np.floor(side_range[<span class="number">0</span>]) / reso)</span><br><span class="line">    y_img += int(np.ceil(fwd_range[<span class="number">1</span>]) / reso)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set height range</span></span><br><span class="line">    height_range = (<span class="number">-2</span>, <span class="number">0.5</span>)</span><br><span class="line">    pixel_val = np.clip(a = z_points, a_min = height_range[<span class="number">0</span>], a_max = height_range[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># scale to 0-255</span></span><br><span class="line">    pixel_val = ((pixel_val - height_range[<span class="number">0</span>]) / float(height_range[<span class="number">1</span>] - height_range[<span class="number">0</span>]) * <span class="number">255</span>).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initial empty image</span></span><br><span class="line">    x_max = int((side_range[<span class="number">1</span>] - side_range[<span class="number">0</span>]) / reso) + <span class="number">1</span> <span class="comment"># cols</span></span><br><span class="line">    y_max = int((fwd_range[<span class="number">1</span>] - fwd_range[<span class="number">0</span>]) / reso) + <span class="number">1</span> <span class="comment"># rows</span></span><br><span class="line">    img = np.zeros([y_max, x_max], dtype = np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># fill image with height info</span></span><br><span class="line">    img[y_img, x_img] = pixel_val</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># show image</span></span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>激光雷达</category>
      </categories>
      <tags>
        <tag>LiDAR</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读(2)--PointNet++</title>
    <url>/2020/01/27/pointnet2/</url>
    <content><![CDATA[<p>论文链接：<a href="https://papers.nips.cc/paper/7095-pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space.pdf" target="_blank" rel="noopener">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></p>
<p>代码：<a href="https://github.com/charlesq34/pointnet2" target="_blank" rel="noopener">tensorflow</a> <a href="https://github.com/erikwijmans/Pointnet2_PyTorch" target="_blank" rel="noopener">pytorch</a></p>
<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h1><p>之前很少有工作研究直接在点集上进行的深度学习，PointNet是这个方向的先驱。但是，PointNet的设计并不能捕获度量空间点的局部结构，限制了它在识别更细密度模式以及概括复杂场景方面的能力。</p>
<p>在本文中，提出了一种分层的神经网络，该网络在输入点集的嵌套分区中递归地使用PointNet(提取特征)。通过使用度量空间距离，该网络可以随着上下文比例的增加来学习局部特征。通过进一步的观察，点集通常会以不同的密度进行采样，这导致以均匀的密度进行训练的网络性能会大大降低，本文提出了新颖的集合学习层，来自适应的组合来自多个尺度的特征。</p>
<p>实验表明，本文提出的PointNet++网络能够高效、鲁棒地学习深度点集特征。特别地，在具有挑战性的3D点云基准测试中，取得了远超过state-of-the-art的结果。</p>
<h1 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2. 网络结构"></a>2. 网络结构</h1><p><img src="/2020/01/27/pointnet2/image-20200128161309179.png" alt="image-20200128161309179"></p>
<p>整体结构如图所示。</p>
<p>PointNet使用单个的最大池化操作来获得整个点集的信息，本文提出的结构构建了点的层次分组，逐层抽象出越来越大的局部区域，该结构包含一系列的点集抽象层，在每一层，通过处理将点集提取为由更少元素组成的新的集合。该点集抽象层主要由三个关键部分组成：采样层、分组层和PointNet层。</p>
<ul>
<li><p>采样层</p>
<p>采样层从输入的点集中选择一部分点构成子集，作为每个局部区域的中心点，本文中选择点的方法用的是迭代最远点采样(FPS)，即每次从剩下的点中选择距离已选点集最远的点作为新加入的点。</p>
</li>
<li><p>组合层</p>
<p>组合层选择每个中心点近邻的点构建局部区域。近邻点的选择一般有两种方法，第一种是K最近邻(kNN)查找，即选取每个点附近距离最近的K个点，K是一个固定的值；第二种是将点附近一定半径球形区域内的点作为近邻点，即ball query。和kNN相比，ball query的局部近邻保证了固定大小的区域尺度，使得空间的局部区域特征可泛化性更强。</p>
</li>
<li><p>PointNet层</p>
<p>使用一个规模较小的PointNet网络将每个局部区域的特征编码成特征向量。</p>
</li>
</ul>
<p>由于点集在不同区域的分布一般是不均匀的，这种特征是点集特征学习的一个挑战。从密集数据学习的特征可能不能概括稀疏的采样区域，同样，用稀疏点云训练的模型也可能识别不了精细的局部结构。</p>
<p>为此，本文提出了密度自适应的PointNet层来学习不同尺度区域的联合特征，并将这种使用密度自适应PointNet的层次网络命名为PointNet++。</p>
<p>在PointNet++中，每个抽象层提取多个尺度的个局部模式，并根据局部点的密度将它们自动结合，本文提出了两种不同的密度自适应层。</p>
<ul>
<li><p>Multi-scale grouping(MSG).</p>
<p><img src="/2020/01/27/pointnet2/image-20200128165447076.png" alt="image-20200128165447076"></p>
<p>一种简单有效的方法就是使用不同尺度的组合层，再用PointNet提取出各个尺度上的特征，将它们串成一个多尺度的特征，如上图所示。</p>
<p>该方法计算量较大，因为它对每个采样的中心点都要在多个尺度上运行PointNet提取特征。</p>
</li>
<li><p>Multi-resolution grouping(MRG)</p>
<p><img src="/2020/01/27/pointnet2/image-20200128165957911.png" alt="image-20200128165957911"></p>
</li>
</ul>
<p>  为了减少计算量，本文又提出了MRG方法，在该方法中，对于某个抽象层的区域特征，可以用两个向量的串联来表示，第一个向量通过使用点集抽象层汇总来自较低一级的每个子区域特征得到，第二个向量直接在局部区域的原始点上使用一个PointNet得到。</p>
<p>  当局部区域的密度比较小时，第二个向量比第一个更值得信赖，因此第二个向量所占的权重应该高一点；反过来，当局部区域密度较高时，第一个向量提供了更详细的信息。</p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h1><p>数据集：</p>
<ul>
<li>MNIST：手写数字图像，60k用于训练，10k用于测试；</li>
<li>ModelNet40：40个类别的CAD模型，9843训练，2468测试；</li>
<li>SHREC15：50个类别的1200个形状，每个类别包含24个形状，采用了5折交叉验证。</li>
<li>ScanNet：1513个室内扫描场景，1201训练，312测试。</li>
</ul>
<h2 id="3-1-欧式度量空间点集分类"><a href="#3-1-欧式度量空间点集分类" class="headerlink" title="3.1 欧式度量空间点集分类"></a>3.1 欧式度量空间点集分类</h2><p>二维使用MINIST数据集，三维使用ModelNet40数据集。</p>
<p><img src="/2020/01/27/pointnet2/image-20200128172123486.png" alt="image-20200128172123486"></p>
<p><img src="/2020/01/27/pointnet2/image-20200128172136852.png" alt="image-20200128172136852"></p>
<p>对于不同采样密度的鲁棒性实验</p>
<p><img src="/2020/01/27/pointnet2/image-20200128172236570.png" alt="image-20200128172236570"></p>
<h2 id="3-2-语义场景的点集分割"><a href="#3-2-语义场景的点集分割" class="headerlink" title="3.2 语义场景的点集分割"></a>3.2 语义场景的点集分割</h2><p>数据集：ScanNet</p>
<p><img src="/2020/01/27/pointnet2/image-20200128172430099.png" alt="image-20200128172430099"></p>
<p><img src="/2020/01/27/pointnet2/image-20200128172501557.png" alt="image-20200128172501557"></p>
<h2 id="3-3-非欧式度量空间的点集分类"><a href="#3-3-非欧式度量空间的点集分类" class="headerlink" title="3.3 非欧式度量空间的点集分类"></a>3.3 非欧式度量空间的点集分类</h2><p>数据集：SHREC15</p>
<p><img src="/2020/01/27/pointnet2/image-20200128172625876.png" alt="image-20200128172625876"></p>
<p><img src="/2020/01/27/pointnet2/image-20200128172638932.png" alt="image-20200128172638932"></p>
<h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>本文在PointNet的基础上提出了分层的网络结构PointNet++，在各个数据集上都取得了较好效果。主要贡献提出对输入点集的嵌套分区，以及对非均匀采样点问题的处理。</p>
<p>未来，值得思考的是如何在局部区域共享更多计算来提高推理速度，尤其是在MSG和MRG层上。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>cls.</tag>
        <tag>seg.</tag>
      </tags>
  </entry>
  <entry>
    <title>论文阅读(1)--PointNet</title>
    <url>/2020/01/27/pointNet/</url>
    <content><![CDATA[<p>论文链接：<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></p>
<p>代码：<a href="https://github.com/charlesq34/pointnet" target="_blank" rel="noopener">tensroflow</a> <a href="https://github.com/fxia22/pointnet.pytorch" target="_blank" rel="noopener">pytorch</a> </p>
<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h1><p>点云是几何数据结构的一种重要类型，由于它的格式不规则，大多数研究人员将这种数据转换成规则的三维体素栅格或者多幅图像的集合来进行处理。但是，这种方式会增加不必要的数据量并且导致一些问题。</p>
<p>在本文中，设计了一种可以直接处理点云的新型神经网络，该网络很好的考虑的点云输入的排列不变性，将该网络命名为PointNet。PointNet为目标检测、部分分割、场景语义解析等应用提供了一种统一的结构。尽管很简单，PointNet非常的高效和有效。从实验上，它表现出与start-of-the-art同等强大或更优的性能；从理论上，本文分析了该网络学到了什么以及为何该网络对输入的扰动和破坏具有鲁棒性。</p>
<h1 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2. 网络结构"></a>2. 网络结构</h1><p><img src="/2020/01/27/pointNet/image-20200127160355226.png" alt="image-20200127160355226"></p>
<p>整体网络结构如图所示，由三个关键模块组成：</p>
<ul>
<li>最大池化层，作为对称的函数来从所有点中生成信息。</li>
</ul>
<p><img src="/2020/01/27/pointNet/image-20200127161302928.png" alt="image-20200127161302928"></p>
<p>​     为了使模型对输入点的排列具有不变性，有三种不同的策略：1）对输入按照某种规则进行排序；2）将所有可能的排列作为一个序列进行输入，训练一个RNN; 3）使用一个具有对称性的函数</p>
<p>​     本文中使用了第三种策略，采用了一个最大池化层对所有点的每个维度进行最大池化处理，生成与输入顺序无关的全局特征。</p>
<ul>
<li>一个局部和全局信息组合的结构(用于seg)</li>
</ul>
<p><img src="/2020/01/27/pointNet/image-20200127161520979.png" alt="image-20200127161520979"></p>
<p>​        在计算出全局特征向量后，在每个点的局部特征之后加上该特征向量生成新的特征，可以使每个点既有局部信息又有全局信息。</p>
<ul>
<li>对齐网络分别用于对齐输入点云和特征(T-Net)</li>
</ul>
<p><img src="/2020/01/27/pointNet/image-20200127161636465.png" alt="image-20200127161636465"></p>
<p>​     使用一个迷你网络(T-Net)来预测一个仿射变换，然后直接将该变换应用到输入点的坐标。同理，对于提取出的特征，也可以用网络学习一个更高维度的变换矩阵，并约束该矩阵为正交矩阵。</p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h1><h2 id="3-1-3D-目标分类"><a href="#3-1-3D-目标分类" class="headerlink" title="3.1 3D 目标分类"></a>3.1 3D 目标分类</h2><p>数据集：ModelNet40 shape classification benchmark。12311个CAD模型共40类，9843用于训练，2468用于测试。</p>
<p><img src="/2020/01/27/pointNet/image-20200127163950411.png" alt="image-20200127163950411"></p>
<h2 id="3-2-3D物体部分分割"><a href="#3-2-3D物体部分分割" class="headerlink" title="3.2 3D物体部分分割"></a>3.2 3D物体部分分割</h2><p>数据集：ShapeNet part data.16个类别的16881个形状，总共有50个部分的标注，大多数类别的物体标注了2-5个部分。</p>
<p><img src="/2020/01/27/pointNet/image-20200127164351963.png" alt="image-20200127164351963"></p>
<h2 id="3-3-场景语义分割"><a href="#3-3-场景语义分割" class="headerlink" title="3.3 场景语义分割"></a>3.3 场景语义分割</h2><p>数据集：Stanford 3D semantic parsing dataset.27个房间6个区域的3D扫描点，共有13个类别的语义类别。</p>
<p><img src="/2020/01/27/pointNet/image-20200127164639253.png" alt="image-20200127164639253"></p>
<p><img src="/2020/01/27/pointNet/image-20200127164650996.png" alt="image-20200127164650996"></p>
<p><img src="/2020/01/27/pointNet/image-20200127164701888.png" alt="image-20200127164701888"></p>
<h2 id="3-4-时空复杂度"><a href="#3-4-时空复杂度" class="headerlink" title="3.4 时空复杂度"></a>3.4 时空复杂度</h2><p><img src="/2020/01/27/pointNet/image-20200127164847958.png" alt="image-20200127164847958"></p>
<h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>本文的工作提出了一种新颖的直接输入点云的网络结构，为多种三维任务提供了统一的方法，取得了较好的结果。</p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>cls.</tag>
        <tag>seg.</tag>
        <tag>det.</tag>
      </tags>
  </entry>
</search>
